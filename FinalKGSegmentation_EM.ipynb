{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw7XZ4yzh7Jm"
      },
      "source": [
        "### **Mount Google Drive/Parent Directory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU-TDjsOcfqG",
        "outputId": "004a3bf3-db52-46d7-8cde-70a64a175835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CECUi9j5wmRu"
      },
      "source": [
        "### **Install and Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BfNELAEi7hv",
        "outputId": "66485650-92d5-4fe8-dfca-06f8cb35b6b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from nibabel) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "# Library to read .nii Images\n",
        "!pip install nibabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "tHB3nA8nwq90"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import copy\n",
        "import numpy as np\n",
        "import numpy as np                                            \n",
        "import matplotlib.pyplot as plt                 \n",
        "from scipy.stats import multivariate_normal\n",
        "import random  \n",
        "import time  \n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.cluster import KMeans\n",
        "# Library to read .nii Images\n",
        "import nibabel as nib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2pnu_Tj1dZ5"
      },
      "source": [
        "**Image Slice Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "kZGtKZDq1bzO"
      },
      "outputs": [],
      "source": [
        "# Visualize 2D Slice from 3D Image\n",
        "def slice_show(image, slice_no, title):\n",
        "    \"\"\"\n",
        "    Inputs: \n",
        "    image => Nifti Image that need to be visualized,\n",
        "    slice_no => Slice Number from 1 to 48\n",
        "    title => The title of the Image\n",
        "    \n",
        "    output: Plot Image.\n",
        "\n",
        "    \"\"\" \n",
        "    plt.figure()\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.imshow(image[:,:,slice_no].T, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation Metric**"
      ],
      "metadata": {
        "id": "Qry8NOwOdBcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tissue_visualization(Seg_CSF, GT_CSF, Seg_GM, GT_GM, Seg_WM, GT_WM, number_of_slice):\n",
        "\n",
        "  \"\"\"\n",
        "  Inputs: \n",
        "  Seg_CSF => CSF class data of the brain tissue after segmentation,\n",
        "  GT_CSF => Ground truth of CSF Class,\n",
        "  Seg_GM => GM class data of the brain tissue after segmentation,\n",
        "  GT_GM => Ground truth of GM Class,\n",
        "  Seg_WM => WM class data of the brain tissue after segmentation,\n",
        "  GT_WM => Ground truth of WM Class,\n",
        "  \n",
        "  output: Plot Images.\n",
        "\n",
        "  \"\"\" \n",
        "\n",
        "  #Visualize each tissue separately\n",
        "  fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots( 2, 3, figsize=(10,6))\n",
        "\n",
        "  ax1.set_title(\"Class CSF #{}\".format(number_of_slice))\n",
        "  img1 = ax1.imshow(np.transpose(Seg_CSF[:,:,number_of_slice]), cmap = \"gray\")\n",
        "  ax2.set_title(\"Class GM #{}\".format(number_of_slice))\n",
        "  img2 = ax2.imshow(np.transpose(Seg_GM[:,:,number_of_slice]), cmap = \"gray\")\n",
        "  ax3.set_title(\"Class WM #{}\".format(number_of_slice))\n",
        "  img3 = ax3.imshow(np.transpose(Seg_WM[:,:,30]), cmap = \"gray\")\n",
        "  ax4.set_title(\"Ground Truth CSF #{}\".format(number_of_slice))\n",
        "  img4 = ax4.imshow(np.transpose(GT_CSF[:,:,number_of_slice]), cmap = \"gray\")\n",
        "  ax5.set_title(\"Ground Truth of GM #{}\".format(number_of_slice))\n",
        "  img5 = ax5.imshow(np.transpose(GT_GM[:,:,number_of_slice]), cmap = \"gray\")\n",
        "  ax6.set_title(\"Ground Truth of WM #{}\".format(number_of_slice))\n",
        "  img6 = ax6.imshow(np.transpose(GT_WM[:,:,number_of_slice]), cmap = \"gray\")\n",
        "  \n",
        "\n",
        "  ax1.axis('off')\n",
        "  ax2.axis('off')\n",
        "  ax3.axis('off')\n",
        "  ax4.axis('off')\n",
        "  ax5.axis('off')\n",
        "  ax6.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "mwsB1CDsez6b"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def diceScoreSimilarity(segmented_image_nifi, ground_truth_nifti):\n",
        "  \n",
        "    \"\"\"\n",
        "    Inputs: \n",
        "    segmented_image_nifi => Segmeneted image in nii format,\n",
        "    ground_truth_nifti => ground truth in nifti format,\n",
        "\n",
        "    output: Dice Score for all the three classes separately.\n",
        "\n",
        "    \"\"\" \n",
        "           \n",
        "    # Compute DICE\n",
        "    def dice(SI, GT):\n",
        "        #   2 * TP / (FN + (2 * TP) + FP)\n",
        "        intersection = np.logical_and(SI, GT)\n",
        "        return 2. * intersection.sum() / (SI.sum() + GT.sum())\n",
        "    \n",
        "    # Dice  for CSF\n",
        "    Seg_CSF = (segmented_image_nifi == 1)\n",
        "    GT_CSF = (ground_truth_nifti == 1)\n",
        "    dice_CSF = dice(Seg_CSF, GT_CSF)\n",
        "    # Dice  for GM\n",
        "    Seg_GM = (segmented_image_nifi == 2)\n",
        "    GT_GM = (ground_truth_nifti == 2)\n",
        "    dice_GM = dice(Seg_GM, GT_GM)\n",
        "    # Dice  for WM\n",
        "    Seg_WM = (segmented_image_nifi == 3)\n",
        "    GT_WM = (ground_truth_nifti == 3)\n",
        "    dice_WM = dice(Seg_WM, GT_WM)\n",
        "\n",
        "    #  Visualize each tissue class separately\n",
        "    number_of_slice = 24\n",
        "    tissue_visualization(Seg_CSF, GT_CSF, Seg_GM, GT_GM, Seg_WM, GT_WM, number_of_slice)\n",
        "\n",
        "    return dice_CSF, dice_GM, dice_WM"
      ],
      "metadata": {
        "id": "4OGZY9A7dEqr"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYfT-yfOZsV2"
      },
      "source": [
        "### **K-means Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "u9-T461puOq4"
      },
      "outputs": [],
      "source": [
        "def KmeansInitialization(brain_data, num_clusters):\n",
        "\n",
        "    \"\"\"\n",
        "    Inputs: \n",
        "    brain_data => extracted brain_data excluding the black background,\n",
        "    num_clusters => number of classes for segmentation ( here 3 )\n",
        "    \n",
        "    output: \n",
        "    mean_generation => mean initialized after k-means (size: N*M where N is the number of the classes and M is the number of modalities used)\n",
        "    covariance_computation => covariance computed after the data segmentation is done on the data, \n",
        "    (covariance for multimodal data, variance for single modal data) (size: N*M*M where N is the number of the classes and M is the number of modalities used)\n",
        "    a_vector => prior probabilities of each class ( size: N * 1 where N is the number of the classes )\n",
        "    Kmeans_predict_updated => labels assigned by Kmeans to each image pixels (X*1 where is X is number of the pixels in the image)\n",
        "\n",
        "    \"\"\" \n",
        "\n",
        "    \"\"\"\n",
        "    Kmeans Clustering is used from sklearn.cluster.KMeans\n",
        "    Input: n_clusters= Number of cluster\n",
        "          K-means++: initial cluster centers for k-mean clustering in a smart way to speed up convergence\n",
        "          Random State :Determines random number generation for centroid initialization. \n",
        "                        Use an int to make the randomness deterministic\n",
        "    Output:\n",
        "          Kmeans_predict= level index\n",
        "          Centroid= Mean\n",
        "    \"\"\" \n",
        "    # K-means Clustering\n",
        "    kmeans=KMeans(n_clusters = num_clusters,  init = 'k-means++', random_state = 0).fit(brain_data)\n",
        "    Kmeans_predict = kmeans.predict(brain_data)\n",
        "    centroids = kmeans.cluster_centers_\n",
        "\n",
        "    # Sorting centroids using T1 weighted mean\n",
        "    centroids_t1 = centroids[:,0]\n",
        "    min_idx = np.argmin(centroids_t1, axis=0)\n",
        "    max_idx = np.argmax(centroids_t1, axis=0)\n",
        "\n",
        "    centroids_updated = np.zeros(centroids.shape)\n",
        "    Kmeans_predict_updated = np.zeros(Kmeans_predict.shape)\n",
        "\n",
        "    # Updating centroid after sorting according to mean values\n",
        "    centroids_updated[0] = centroids[min_idx]\n",
        "    centroids_updated[2] = centroids[max_idx]\n",
        "\n",
        "    for i in [0,1,2]:\n",
        "      if (min_idx != i and max_idx != i):\n",
        "        mid_idx = i\n",
        "        centroids_updated[1] = centroids[i]\n",
        "\n",
        "    # Making the levels according to the ground truth (not starting from zero)\n",
        "    Kmeans_predict_updated[Kmeans_predict==min_idx] = 1\n",
        "    Kmeans_predict_updated[Kmeans_predict==mid_idx] = 2 \n",
        "    Kmeans_predict_updated[Kmeans_predict==max_idx] = 3\n",
        "\n",
        "    # Assigning classes to tissues\n",
        "    CSF_tissue = brain_data[Kmeans_predict_updated == 1]\n",
        "    GM_tissue = brain_data[Kmeans_predict_updated == 2]\n",
        "    WM_tissue = brain_data[Kmeans_predict_updated == 3]\n",
        "\n",
        "    # Computing mean and covariance\n",
        "    mean_CSF = np.mean(CSF_tissue, axis = 0)\n",
        "    mean_GM = np.mean(GM_tissue, axis = 0)\n",
        "    mean_WM = np.mean(WM_tissue , axis = 0)\n",
        "    cov_CSF = np.cov(CSF_tissue, rowvar = False)\n",
        "    cov_GM = np.cov(GM_tissue, rowvar = False)\n",
        "    cov_WM = np.cov(WM_tissue , rowvar = False)\n",
        "\n",
        "    mean_generation = np.vstack((mean_CSF, mean_GM, mean_WM))\n",
        "    covariance_computation = np.stack((cov_CSF, cov_GM, cov_WM))\n",
        "\n",
        "    # Computing prior probabilities of each class\n",
        "    pp_CSF = CSF_tissue.shape[0]/brain_data.shape[0]\n",
        "    pp_GM = GM_tissue.shape[0]/brain_data.shape[0]\n",
        "    pp_WM = WM_tissue.shape[0]/brain_data.shape[0]\n",
        "\n",
        "    a_vector = np.vstack((pp_CSF, pp_GM, pp_WM))\n",
        "\n",
        "    return mean_generation, covariance_computation, a_vector, Kmeans_predict_updated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3ImSbYIxYN9"
      },
      "source": [
        "### **EM Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialization**"
      ],
      "metadata": {
        "id": "90fb0lsY3BVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parameters_initialization(clusters, choice, meta_data_dice, no_modalities ):\n",
        "\n",
        "  \"\"\"\n",
        "  Inputs: \n",
        "  clusters : number of clusters (here 3)\n",
        "  choice: choice of initilization, 1 for Kmeans, 0 for Random,\n",
        "  meta_data_dice: Dictionary containing all the brain image data to be used in EM\n",
        "\n",
        "  output: mean, covariance and priors after initialization either by Kmeans or Random.\n",
        "\n",
        "  \"\"\" \n",
        "  #Extracting the brain image data to be used for the algorithm\n",
        "  x = meta_data_dice['brain_data']\n",
        "  brain_indices = meta_data_dice['brain_indices']\n",
        "  t1_data = meta_data_dice['t1_data']\n",
        "  feature_data = meta_data_dice['feature_data']\n",
        "  brain_image_nifti = meta_data_dice['brain_image_nifti']\n",
        "  ground_truth = meta_data_dice['ground_truth']\n",
        "  \n",
        "  if choice == 1:\n",
        "\n",
        "    # K-means Clustering Initialization\n",
        "    mean_generation, covariance_computation, a_vector, Kmeans_predict_updated = KmeansInitialization(x, clusters)\n",
        "\n",
        "    # print(covariance_computation.shape)\n",
        "\n",
        "    # Clusteres Image Generation By Kmeans\n",
        "    clustered_data =  np.zeros(feature_data.shape[0])\n",
        "    clustered_data[brain_indices] = Kmeans_predict_updated\n",
        "    clustered_img = np.reshape(clustered_data, brain_image_nifti.shape)\n",
        "\n",
        "    # Saving the images in Nifti Format\n",
        "    brain_seg_data = nib.Nifti1Image(clustered_img, t1_data.affine, t1_data.header)\n",
        "    brain_seg_nifti = brain_seg_data.get_fdata()\n",
        "\n",
        "    # Image Visualization after Kmeans Clustering\n",
        "    titlek = 'Image after Kmeans'\n",
        "    slice_show(brain_seg_nifti, 24, titlek)\n",
        "\n",
        "    # Evaluation using DICE Score\n",
        "    print(\" After applying K-means :\")\n",
        "    dice_CSF, dice_GM, dice_WM = diceScoreSimilarity(brain_seg_nifti, ground_truth)\n",
        "    print(\"CSF DICE = {}\".format(dice_CSF), \"GM DICE = {}\".format(dice_GM), \"WM DICE = {}\".format(dice_WM))\n",
        "    print('\\n')\n",
        "\n",
        "  elif choice == 2:\n",
        "    mean_generation = np.zeros((clusters, no_modalities))  # initializing the mean \n",
        "    covariance_computation = np.zeros((clusters, no_modalities, no_modalities))  # initializing the cov\n",
        "    for k in range(clusters):\n",
        "      r = random.sample(range(0, x.shape[0]), clusters)  # getting random coordinates\n",
        "      mean_generation[k] = x[r[k]]  # getting random data points as the mean\n",
        "      temp = np.random.normal(0, 1000, size=(no_modalities, no_modalities))\n",
        "      covariance_computation[k] = np.dot(temp.transpose(), temp)  # computing the covariance matrix\n",
        "    print(covariance_computation)  \n",
        "    #Initialization of the mixture weights\n",
        "    a_values= 1/clusters\n",
        "    a_vector= np.full(clusters, a_values)\n",
        "\n",
        "    #Reshaping of the mixture weights vector to make it a column vector\n",
        "\n",
        "    a_vector= a_vector.reshape((clusters,-1))\n",
        "\n",
        "      \n",
        "  parameters= {'mean': mean_generation, 'covariance': covariance_computation,\n",
        "              'mixture_weights': a_vector}\n",
        "\n",
        "  #The function returns the generated mean, covariance computed with respect to\n",
        "  #the image data and the mixture weights vector. \n",
        "  #All parameters are returned \n",
        "  return parameters"
      ],
      "metadata": {
        "id": "ODOlqX7mZD7Z"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expectation**"
      ],
      "metadata": {
        "id": "3ew5EGOu3EFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ExpectationStep(gmm):\n",
        "\n",
        "  \"\"\"\n",
        "  Inputs: \n",
        "  gmm => matrix containing the gaussian mixture models (probability density functions).\n",
        "  The size of this matrix is (N,clusters), where N corresponds to the number of datapoints for each\n",
        "  one of the modalities\n",
        "  and K corresponds to the number of clusters (tissues to be segmented)\n",
        "  \n",
        "  output: weights_vector=> Updated membership weights for all data points \n",
        "\n",
        "  \"\"\" \n",
        "\n",
        "  #Parameter gmm: Mixture model for the current datapoint\n",
        "  numerator= gmm \n",
        "  #Denominator of the equation\n",
        "  denominator= np.sum(gmm, axis=1)\n",
        "  denominator= denominator.reshape((denominator.shape[0],1))\n",
        "  #Update of membership weights\n",
        "  weights_vector= numerator/denominator\n",
        "\n",
        "  return weights_vector"
      ],
      "metadata": {
        "id": "--41XHhqeznZ"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maximization**"
      ],
      "metadata": {
        "id": "JvRQ4qQ83Gde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MaximizationStep(x,weights_vector):\n",
        "\n",
        "  \"\"\"\n",
        "  Inputs: \n",
        "  x=> dataset vector whose shape is (N,M), where N correspond to the vector of\n",
        "  datapoints for each modality and M corresponds to the number of modalities\n",
        "  weights_vector => membership weights for all datapoints \n",
        "\n",
        "  output: new_parameters => dictionary containing updated parameters for the current\n",
        "  iteration: alpha (mixture weights), mean and covariance.\n",
        "\n",
        "  \"\"\" \n",
        "\n",
        "  #Number of datapoints for each modality\n",
        "  N= x.shape[0]\n",
        "  #Sum of membership weights (numerator of new mixture weights equation)\n",
        "  N_k= np.sum(weights_vector, axis=0) \n",
        "  #The sum of membership weights is reshaped so that it has the dimension \n",
        "  # clusters,1\n",
        "  N_k = N_k.reshape((N_k.shape[0],1))\n",
        "  #New mixture weights \n",
        "  new_a= N_k/N\n",
        "\n",
        "  #Update of the means\n",
        "  #Following the formulas, dot product between weights vector and datapoints is\n",
        "  #performed.\n",
        "  new_mean = np.dot(np.transpose(weights_vector), x)\n",
        "  new_mean = new_mean/N_k\n",
        "\n",
        "  #Update of the covariances\n",
        "\n",
        "  #Number of clusters\n",
        "  clusters= new_mean.shape[0]\n",
        "  #Number of modalities\n",
        "  M = new_mean.shape[1]\n",
        "  \n",
        "  #New covariance matrix initialization\n",
        "  new_covariance= np.zeros((clusters,M,M))\n",
        "\n",
        "  #Calculation of new covariance matrix for each cluster based on formulas\n",
        "  for c in range(clusters):\n",
        "    formula_first_term=(x-new_mean[c])\n",
        "    formula_second_term= (formula_first_term.transpose()*(weights_vector[:,c].reshape((1,N))))\n",
        "    new_covariance[c] = np.dot(formula_second_term,formula_first_term)\n",
        "    new_covariance[c] = (1/N_k[c]) * new_covariance[c]\n",
        "  \n",
        "  new_parameters= {'new_alpha': new_a, 'new_mean': new_mean, 'new_covariance': new_covariance}\n",
        "\n",
        "  return new_parameters"
      ],
      "metadata": {
        "id": "Ib7EyKPzi92y"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EM Function**"
      ],
      "metadata": {
        "id": "z3hJ7yxC3Lik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EM(max_it,error_tolerance,clusters,kmeans_init, meta_data_dice, no_modalities):\n",
        "\n",
        "  \"\"\"\n",
        "  Inputs: \n",
        "  max_it=> maximum numbers of iterations for the algorithm\n",
        "  error_tolerance => stopping criterion for checking convergence of the algorithm\n",
        "  clusters => number of tissues to be segmented\n",
        "  kmeans_init => boolean variable used as a flag to select the type of parameters initialization\n",
        "  if kmeans_init = True, initialization is taken from K-means algorithm. If kmeans_init = False,\n",
        "  initialization is done randomly. \n",
        "  meta_data_dice => Dictionary containing feature_data (flattened image after skull stripping), brain_indices (non zero indices after excluding background),\n",
        "  brain_data (brain data after excluding background ), brain_img_nifti (original image of the brain in nii format ) \n",
        "\n",
        "  output: final_results \n",
        "\n",
        "  \"\"\" \n",
        "  n_it= 1\n",
        "  x = meta_data_dice['brain_data']\n",
        "  #Number of voxels in the modality\n",
        "  N= x.shape[0]\n",
        "  #Number of modalities\n",
        "  M= x.shape[1]\n",
        "  #Selection of initialization:\n",
        "  if kmeans_init == True:\n",
        "    initial_parameters = parameters_initialization(clusters, 1, meta_data_dice, no_modalities)\n",
        "    # print(initial_parameters)\n",
        "  else:\n",
        "    initial_parameters = parameters_initialization(clusters, 2, meta_data_dice, no_modalities)\n",
        "    # print(initial_parameters)\n",
        "    \n",
        "  #Covariance matrix and mean are recovered from initialization of parameters\n",
        "  covariance_matrix= initial_parameters['covariance']\n",
        "  mean_matrix= initial_parameters['mean']\n",
        "\n",
        "  #Mixture weights are recovered from initialization parameters\n",
        "  a_vector= initial_parameters['mixture_weights']\n",
        "\n",
        "  #Number of clusters\n",
        "  clusters=mean_matrix.shape[0]\n",
        "\n",
        "  #Generation of the matrix to store the Gaussian mixture models. This matrix \n",
        "  #will be of size NxK number of voxels in the modality x number of clusters\n",
        "  gmm = np.zeros((N,clusters))\n",
        "    \n",
        "\n",
        "  #First Step: Computation of Gaussian mixture model for each tissue\n",
        "  for c in range(clusters):\n",
        "    covariance_matrix[c] = covariance_matrix[c] \n",
        "    gmm[:,c] = multivariate_normal.pdf(x, mean_matrix[c], covariance_matrix[c] )\n",
        "    gmm[:,c] =  gmm[:,c]*a_vector[c]\n",
        "  \n",
        "  # After the computation of the Gaussian Mixture Model for each tissue, the \n",
        "  # initial likelihood is computed \n",
        "\n",
        "  c_llhd = np.log(gmm.sum(axis=1)).sum()\n",
        "\n",
        "  # EM algorithm\n",
        "\n",
        "  while n_it<= max_it:\n",
        "    print('The algorithm is running iteration # ', n_it)\n",
        "\n",
        "    #EXPECTATION STEP\n",
        "\n",
        "    weights_vector= ExpectationStep(gmm)\n",
        "\n",
        "    #MAXIMIZATION STEP\n",
        "\n",
        "    new_parameters= MaximizationStep(x, weights_vector)\n",
        "\n",
        "    # UPDATE OF GMM\n",
        "\n",
        "    #Covariance matrix and mean are recovered from new parameters from the \n",
        "    #maximization step \n",
        "    covariance_matrix=  new_parameters['new_covariance']\n",
        "    mean_matrix= new_parameters['new_mean']\n",
        "\n",
        "    #Mixture weights are recovered from new parameters from \n",
        "    a_vector= new_parameters['new_alpha']\n",
        "\n",
        "    #Number of clusters\n",
        "    clusters=mean_matrix.shape[0]\n",
        "\n",
        "    #Generation of the matrix to store the Gaussian mixture models. This matrix \n",
        "    #will be of size NxK number of voxels in the modality x number of clusters\n",
        "    gmm = np.zeros((N,clusters))\n",
        "    \n",
        "\n",
        "    #Computation of Gaussian mixture model for each tissue\n",
        "    for uc in range(clusters):\n",
        "      covariance_matrix[uc] = covariance_matrix[uc] + np.eye(M)*1e-10\n",
        "      gmm[:,uc] = multivariate_normal.pdf(x, mean_matrix[uc], covariance_matrix[uc])\n",
        "      gmm[:,uc] =  gmm[:,uc]*a_vector[uc]\n",
        "\n",
        "    #New loglikelihood\n",
        "\n",
        "    prev_llhd= c_llhd\n",
        "\n",
        "    c_llhd= np.log(gmm.sum(axis=1)).sum()\n",
        "\n",
        "    #Stopping criterion: absolute differences of loglikelihood\n",
        "\n",
        "    #Printing absolute difference of loglikelihood\n",
        "    print(abs(c_llhd-prev_llhd))\n",
        "\n",
        "\n",
        "    if(abs(c_llhd-prev_llhd)< error_tolerance):\n",
        "      print('The algorithm has converged')\n",
        "      print('The total number of iterations until convergence was:', n_it)\n",
        "      final_results={'mean': mean_matrix, 'gmm': gmm,\n",
        "                     'covariance': covariance_matrix, 'iterations': n_it}\n",
        "      return final_results\n",
        "\n",
        "    n_it= n_it+1\n",
        "  \n",
        "  print('The maximum number of iterations was reached')\n",
        "  final_results={'mean': mean_matrix, 'gmm': gmm,\n",
        "                     'covariance': covariance_matrix, 'iterations': n_it}\n",
        "  return final_results      "
      ],
      "metadata": {
        "id": "RRK3puYdJLsn"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Image Reconstruction**"
      ],
      "metadata": {
        "id": "vLJ7muJl4IhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ImageReconstruction(em_results, meta_data_dice):\n",
        "\n",
        "  \"\"\"\n",
        "  Inputs: \n",
        "  em_results => dictionary containing resulting value of the parameters ( mean, cov, gmm ) after EM\n",
        "  meta_data_dice => Dictionary containing all the brain image data to be used in EM\n",
        "\n",
        "  output: Plot Images adn Get Dice Scores.\n",
        "\n",
        "  \"\"\" \n",
        "\n",
        "  brain_indices = meta_data_dice['brain_indices']\n",
        "  t1_data = meta_data_dice['t1_data']\n",
        "  feature_data = meta_data_dice['feature_data']\n",
        "  brain_image_nifti = meta_data_dice['brain_image_nifti']\n",
        "  ground_truth = meta_data_dice['ground_truth']\n",
        "\n",
        "  seg_prob = em_results['gmm']\n",
        "  centroids = em_results['mean']\n",
        "  seg_labels = np.argmax(seg_prob, axis=1)\n",
        "\n",
        "  # Sorting centroids using T1 weighted mean\n",
        "  centroids_t1 = centroids[:,0]\n",
        "  min_idx = np.argmin(centroids_t1, axis=0)\n",
        "  max_idx = np.argmax(centroids_t1, axis=0)\n",
        "\n",
        "  centroids_updated = np.zeros(centroids.shape)\n",
        "  seg_labels_updated = np.zeros(seg_labels.shape)\n",
        "\n",
        "  # Updating centroid after sorting according to mean values\n",
        "  centroids_updated[0] = centroids[min_idx]\n",
        "  centroids_updated[2] = centroids[max_idx]\n",
        "\n",
        "  for i in [0,1,2]:\n",
        "    if (min_idx != i and max_idx != i):\n",
        "      mid_idx = i\n",
        "      centroids_updated[1] = centroids[i]\n",
        "\n",
        "  # Making the levels according to the ground truth (not starting from zero)\n",
        "  seg_labels_updated[seg_labels==min_idx] = 1\n",
        "  seg_labels_updated[seg_labels==mid_idx] = 2 \n",
        "  seg_labels_updated[seg_labels==max_idx] = 3\n",
        "\n",
        "\n",
        "  seg_data =  np.zeros(feature_data.shape[0])\n",
        "  seg_data[brain_indices] = seg_labels_updated\n",
        "  seg_result = np.reshape(seg_data, brain_image_nifti.shape)\n",
        "  # Saving the images in Nifti Format\n",
        "  brain_seg_data = nib.Nifti1Image(seg_result, t1_data.affine, t1_data.header)\n",
        "  brain_seg_nifti = brain_seg_data.get_fdata()\n",
        "  titleem = 'Image after EM'\n",
        "  slice_show(brain_seg_nifti, 24, titleem)\n",
        "  # Evaluation using DICE Score\n",
        "  print('\\nAfter applying EM:')\n",
        "  dice_CSF, dice_GM, dice_WM = diceScoreSimilarity(brain_seg_nifti, ground_truth)\n",
        "  print(\"CSF DICE = {}\".format(dice_CSF), \"GM DICE = {}\".format(dice_GM), \"WM DICE = {}\".format(dice_WM))"
      ],
      "metadata": {
        "id": "On80G9RH45_3"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hSo1hEEwxCV"
      },
      "source": [
        "### **Skull Stripping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "5Wtg7J4rwzms"
      },
      "outputs": [],
      "source": [
        "def skull_stripping(input_img, labeled_img):\n",
        "  \"\"\"\n",
        "  Inputs: \n",
        "  input_img : Original given image with skull\n",
        "  labeled_img : ground_truth\n",
        "\n",
        "  output: stripped skull image.\n",
        "\n",
        "  \"\"\" \n",
        "  labeled_img[labeled_img>0]=1\n",
        "  result = np.multiply(input_img, labeled_img)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Workflow**"
      ],
      "metadata": {
        "id": "r0BSY6DB93iP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_NLfXEdj9WH"
      },
      "source": [
        "### **NIFTI Image Read and Load**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loadImagedata(patient_no, modality_list):\n",
        "\n",
        "  \"\"\"\n",
        "  Inputs: \n",
        "  patient_no : patient number from 1 to 5\n",
        "  modality_list : A list containing the image modality names to be used\n",
        "\n",
        "  output: \n",
        "  meta_data_dice : Dictionary containing feature_data (flattened image after skull stripping), brain_indices (non zero indices after excluding background),\n",
        "  brain_data (brain data after excluding background ), brain_img_nifti (original image of the brain in nii format ) \n",
        "\n",
        "  \"\"\" \n",
        "  \n",
        "  # Read Nifti Images\n",
        "  slice_no = 24\n",
        "  input_directory = '/content/drive/MyDrive/Colab Notebooks/MISA-MIRA/P2_data'\n",
        "  patient = input_directory + '/' + str(patient_no)\n",
        "\n",
        "  ground_data = nib.load(patient +'/LabelsForTesting.nii')\n",
        "  ground_truth = ground_data.get_fdata()\n",
        "  labeled_img = ground_truth.copy()\n",
        "  # Visualize ground truth\n",
        "  titleG = 'Ground Truth'\n",
        "  slice_show(labeled_img, slice_no, titleG)\n",
        "  flattened_list = []\n",
        "\n",
        "  t1_data = nib.load(patient + '/'+modalities_list[0])\n",
        "\n",
        "  for modality in modalities_list:\n",
        "    data = nib.load(patient + '/'+modality)\n",
        "    img = data.get_fdata()\n",
        "    # Visualize the input images\n",
        "    title1 = 'Original {} Image:'.format(modality)\n",
        "    slice_show(img, slice_no, title1)\n",
        "    brain_image = skull_stripping(img, labeled_img)\n",
        "    # Saving the images in Nifti Format\n",
        "    brain_image_data = nib.Nifti1Image(brain_image, data.affine, data.header)\n",
        "    brain_image_nifti = brain_image_data.get_fdata()\n",
        "    # Visualize the input images after skull stripping\n",
        "    title2 = 'Skull-Stripped {} Image:'.format(modality)\n",
        "    slice_show(brain_image_nifti, slice_no, title2)\n",
        "    # Flattening and concatenating image modalities\n",
        "    flattened_brain_image = brain_image_nifti.copy().flatten()\n",
        "    flattened_list.append(flattened_brain_image)\n",
        "\n",
        "  feature_data = flattened_list[0]\n",
        "  for i in range(1, len(flattened_list)):\n",
        "    feature_data = np.vstack((feature_data, flattened_list[i] ))\n",
        "  \n",
        "  feature_data = np.transpose(feature_data) \n",
        "  if(len(modality_list)==1):\n",
        "    feature_data = feature_data.reshape((feature_data.shape[0],1))\n",
        "    \n",
        "  \n",
        "  # Extracting black background from the image data considering only brain\n",
        "  brain_indices = []\n",
        "  for index, data in enumerate(feature_data):\n",
        "    if data.any():\n",
        "      brain_indices.append(index)\n",
        "  # Brain Data excluding background pixels\n",
        "  brain_data = feature_data[brain_indices]\n",
        "\n",
        "  meta_data_dice = {'brain_indices': brain_indices, 't1_data': t1_data, 'feature_data': feature_data, 'brain_data': brain_data,\n",
        "            'brain_image_nifti': brain_image_nifti, 'ground_truth': ground_truth}\n",
        "  return meta_data_dice\n"
      ],
      "metadata": {
        "id": "-4NgoFgnPLJl"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EM**"
      ],
      "metadata": {
        "id": "kE0dqiZxXScs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EM Results\n",
        "Patient_No = 5 # Patient Number can be from 1 to 5\n",
        "modalities_list = ['T1.nii', 'T2_FLAIR.nii'] # List of Image Modalities\n",
        "kmeans_init = True # Initialization by Kmeans (True) or Random(False)\n",
        "meta_data_dice = loadImagedata(Patient_No, modalities_list)\n",
        "max_it=500 # Number of maximum Iterations to be allowed\n",
        "error_tolerance=0.001 # Tolarance of Likelihood for the convergence \n",
        "clusters = 3 # Number of tissue classes\n",
        "no_modalities = len(modalities_list)\n",
        "print('For Patient-{}'.format(Patient_No))\n",
        "start_time = time.time()\n",
        "em_results = EM(max_it, error_tolerance, clusters, kmeans_init, meta_data_dice, no_modalities) #Call EM Algorithm\n",
        "ImageReconstruction(em_results, meta_data_dice) # Generate the dice scores and segmented images\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print('Total time for the computation : {}s'.format(total_time))"
      ],
      "metadata": {
        "id": "xNjw0B4HkTKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.random.normal(0, 1, size=(3,)))"
      ],
      "metadata": {
        "id": "W1rya7Fkc9Lw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Lw7XZ4yzh7Jm",
        "Qry8NOwOdBcW",
        "nYfT-yfOZsV2",
        "vLJ7muJl4IhA",
        "_hSo1hEEwxCV",
        "5_NLfXEdj9WH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}